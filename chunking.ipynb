{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "\n",
    "with open('dataset/alqac25_private_test_task2_combined.json', 'r', encoding='utf-8') as f:\n",
    "    private_test = json.load(f)\n",
    "\n",
    "with open(\"dataset/alqac25_law.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    laws = json.load(f)\n",
    "\n",
    "\n",
    "article_mapping = {\n",
    "    item['id']: {article[\"id\"]: article for article in item[\"articles\"]}\n",
    "    for item in laws\n",
    "}\n",
    "\n",
    "def get_text(law_id, article_id):\n",
    "    law_id = str(law_id)\n",
    "    article_id = str(article_id)        \n",
    "    text = article_mapping.get(law_id, {}).get(article_id, {}).get(\"text\", \"\")\n",
    "    if not text:\n",
    "        raise ValueError(f\"Article {article_id} of Law {law_id} not found.\")\n",
    "    return text\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(separators=[\n",
    "                                          \"\\n\\n\", r\"\\n\\d+\\. Sửa đổi\", \"\\n\\d+\", \"\\n\", \" \", \"\"], is_separator_regex=True, chunk_size=500, chunk_overlap=100)\n",
    "split_texts = {}\n",
    "\n",
    "for law_id, articles in article_mapping.items():\n",
    "    for article_id in articles:\n",
    "        if law_id not in split_texts:\n",
    "            split_texts[law_id] = {}\n",
    "        split_texts[law_id][article_id] = splitter.split_text(\n",
    "            get_text(law_id, article_id))\n",
    "\n",
    "def get_text(law_id, article_id):\n",
    "    law_id = str(law_id)\n",
    "    article_id = str(article_id)        \n",
    "    text = article_mapping.get(law_id, {}).get(article_id, {}).get(\"text\", \"\")\n",
    "    if not text:\n",
    "        raise ValueError(f\"Article {article_id} of Law {law_id} not found.\")\n",
    "def process_relevant_articles(articles):\n",
    "    results = []\n",
    "    for item in articles:\n",
    "        law_id = item['law_id']\n",
    "        article_id = item['article_id']\n",
    "        texts = split_texts[law_id][article_id]\n",
    "        for idx, text in enumerate(texts):\n",
    "            results.append({\n",
    "                'law_id': law_id,\n",
    "                'article_id': article_id,\n",
    "                'text': text,\n",
    "                'index': idx\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "for item in private_test:\n",
    "    item['relevant_articles'] = process_relevant_articles(\n",
    "        item['relevant_articles'])\n",
    "\n",
    "with open('dataset/alqac25_private_test_task2_combined_processed.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(private_test, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
